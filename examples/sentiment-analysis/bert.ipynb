{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0a4mTk9o1Qg"
      },
      "source": [
        "# Modified source from https://colab.research.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb\n",
        "\n",
        "# Copyright 2019 Google Inc.\n",
        "\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCpvgG0vwXAZ"
      },
      "source": [
        "#Predicting Movie Review Sentiment with BERT on TF Hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiYrZKaHwV81"
      },
      "source": [
        "If you’ve been following Natural Language Processing over the past year, you’ve probably heard of BERT: Bidirectional Encoder Representations from Transformers. It’s a neural network architecture designed by Google researchers that’s totally transformed what’s state-of-the-art for NLP tasks, like text classification, translation, summarization, and question answering.\n",
        "\n",
        "Now that BERT's been added to [TF Hub](https://www.tensorflow.org/hub) as a loadable module, it's easy(ish) to add into existing TensorFlow text pipelines. In an existing pipeline, BERT can replace text embedding layers like ELMO and GloVE. Alternatively, [finetuning](http://wiki.fast.ai/index.php/Fine_tuning) BERT can provide both an accuracy boost and faster training time in many cases.\n",
        "\n",
        "Here, we'll train a model to predict whether an IMDB movie review is positive or negative using BERT in TensorFlow with tf hub. Some code was adapted from [this colab notebook](https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb). Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chM4UttbMIqq"
      },
      "source": [
        "First, we'll install the required packages:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jviywGyWyKsA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acab7c04-ea9b-4346-9944-c97d3b1fc58b"
      },
      "source": [
        "!pip install bert-tensorflow==1.0.* tensorflow-gpu==1.13.* scikit-learn==0.21.* pandas==0.24.* tensorflow-hub==0.6.* boto3==1.*"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bert-tensorflow==1.0.*\n",
            "  Downloading bert_tensorflow-1.0.4-py2.py3-none-any.whl.metadata (619 bytes)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow-gpu==1.13.* (from versions: 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0, 2.12.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow-gpu==1.13.*\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip bert-master"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Q_49m-Dgktt",
        "outputId": "77009c8e-cba8-43ec-b209-cfce095c4217"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  bert-master.zip\n",
            "eedf5716ce1268e56f0a50264a88cafad334ac61\n",
            "   creating: bert-master/\n",
            "  inflating: bert-master/.gitignore  \n",
            "  inflating: bert-master/CONTRIBUTING.md  \n",
            "  inflating: bert-master/LICENSE     \n",
            "  inflating: bert-master/README.md   \n",
            "  inflating: bert-master/__init__.py  \n",
            "  inflating: bert-master/create_pretraining_data.py  \n",
            "  inflating: bert-master/extract_features.py  \n",
            "  inflating: bert-master/modeling.py  \n",
            "  inflating: bert-master/modeling_test.py  \n",
            "  inflating: bert-master/multilingual.md  \n",
            "  inflating: bert-master/optimization.py  \n",
            "  inflating: bert-master/optimization_test.py  \n",
            "  inflating: bert-master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb  \n",
            "  inflating: bert-master/requirements.txt  \n",
            "  inflating: bert-master/run_classifier.py  \n",
            "  inflating: bert-master/run_classifier_with_tfhub.py  \n",
            "  inflating: bert-master/run_pretraining.py  \n",
            "  inflating: bert-master/run_squad.py  \n",
            "  inflating: bert-master/sample_text.txt  \n",
            "  inflating: bert-master/tokenization.py  \n",
            "  inflating: bert-master/tokenization_test.py  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mv bert-master bert"
      ],
      "metadata": {
        "id": "cJfRW2pi_gWa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsZvic2YxnTz"
      },
      "source": [
        "from datetime import datetime\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import sys\n",
        "sys.path.append('.')\n",
        "sys.path.append('./bert')\n",
        "import bert\n",
        "# from bert import run_classifier\n",
        "# from bert import optimization\n",
        "from bert import tokenization\n",
        "\n",
        "from bert import modeling\n",
        "# from bert import optimization\n",
        "from bert.modeling import create_attention_mask_from_input_mask, transformer_model, get_activation\n",
        "from tensorflow.python.keras.layers import Dense"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVB3eOcjxxm1"
      },
      "source": [
        "Below, we'll set an output location to store our model output, checkpoints, and export in a local directory. Note: if you're running on Google Colab, local directories don't persist after the session ends."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "US_EAnICvP7f"
      },
      "source": [
        "OUTPUT_DIR = \"bert\"\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmFYvkylMwXn"
      },
      "source": [
        "#Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MC_w8SRqN0fr"
      },
      "source": [
        "First, let's download the dataset, hosted by Stanford. The code below, which downloads, extracts, and imports the IMDB Large Movie Review Dataset, is borrowed from [this TensorFlow tutorial](https://www.tensorflow.org/hub/tutorials/text_classification_with_tf_hub)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fom_ff20gyy6"
      },
      "source": [
        "from tensorflow import keras\n",
        "import os\n",
        "import re\n",
        "\n",
        "# Load all files from a directory in a DataFrame.\n",
        "def load_directory_data(directory):\n",
        "  data = {}\n",
        "  data[\"sentence\"] = []\n",
        "  data[\"sentiment\"] = []\n",
        "  for file_path in os.listdir(directory):\n",
        "    #with tf.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:#tf.io.gfile.GFile\n",
        "    with tf.io.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n",
        "      data[\"sentence\"].append(f.read())\n",
        "      data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n",
        "  return pd.DataFrame.from_dict(data)\n",
        "\n",
        "# Merge positive and negative examples, add a polarity column and shuffle.\n",
        "def load_dataset(directory):\n",
        "  pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n",
        "  neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n",
        "  pos_df[\"polarity\"] = 1\n",
        "  neg_df[\"polarity\"] = 0\n",
        "  return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Download and process the dataset files.\n",
        "def download_and_load_datasets(force_download=False):\n",
        "  dataset = tf.keras.utils.get_file(\n",
        "      fname=\"aclImdb.tar.gz\",\n",
        "      origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\",\n",
        "      extract=True)\n",
        "\n",
        "  train_df = load_dataset(os.path.join(os.path.dirname(dataset),\n",
        "                                       \"aclImdb\", \"train\"))\n",
        "  test_df = load_dataset(os.path.join(os.path.dirname(dataset),\n",
        "                                      \"aclImdb\", \"test\"))\n",
        "\n",
        "  return train_df, test_df\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2abfwdn-g135"
      },
      "source": [
        "train, test = download_and_load_datasets()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XA8WHJgzhIZf"
      },
      "source": [
        "To keep training fast, we'll take a sample of 5000 train and test examples, respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lw_F488eixTV"
      },
      "source": [
        "train = train.sample(5000)\n",
        "test = test.sample(5000)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prRQM8pDi8xI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4d9651a-47b7-4a99-e931-f72a68303bd2"
      },
      "source": [
        "train.columns"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['sentence', 'sentiment', 'polarity'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfRnHSz3iSXz"
      },
      "source": [
        "For us, our input data is the 'sentence' column and our label is the 'polarity' column (0, 1 for negative and positive, respecitvely)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuMOGwFui4it"
      },
      "source": [
        "DATA_COLUMN = 'sentence'\n",
        "LABEL_COLUMN = 'polarity'\n",
        "# label_list is the list of labels, i.e. True, False or 0, 1 or 'dog', 'cat'\n",
        "label_list = [0, 1]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V399W0rqNJ-Z"
      },
      "source": [
        "#Data Preprocessing\n",
        "We'll need to transform our data into a format BERT understands. This involves two steps. First, we create  `InputExample`'s using the constructor provided in the BERT library.\n",
        "\n",
        "- `text_a` is the text we want to classify, which in this case, is the `Request` field in our Dataframe.\n",
        "- `text_b` is used if we're training a model to understand the relationship between sentences (i.e. is `text_b` a translation of `text_a`? Is `text_b` an answer to the question asked by `text_a`?). This doesn't apply to our task, so we can leave `text_b` blank.\n",
        "- `label` is the label for our example, i.e. True, False"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9gEt5SmM6i6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "39331721-d0ec-4ada-f7a5-08533573906f"
      },
      "source": [
        "# Use the InputExample class from BERT's run_classifier code to create examples from the data\n",
        "train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
        "                                                                   text_a = x[DATA_COLUMN],\n",
        "                                                                   text_b = None,\n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "test_InputExamples = test.apply(lambda x: bert.run_classifier.InputExample(guid=None,\n",
        "                                                                   text_a = x[DATA_COLUMN],\n",
        "                                                                   text_b = None,\n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'bert' has no attribute 'run_classifier'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-d5e2124006c8>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Use the InputExample class from BERT's run_classifier code to create examples from the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n\u001b[0m\u001b[1;32m      3\u001b[0m                                                                    \u001b[0mtext_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDATA_COLUMN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                                                    \u001b[0mtext_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                                                    label = x[LABEL_COLUMN]), axis = 1)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m  10372\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10373\u001b[0m         )\n\u001b[0;32m> 10374\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"apply\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  10375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10376\u001b[0m     def map(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    914\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 916\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1061\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m             \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1064\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m             \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_series_numba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1079\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m                 \u001b[0;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m                 \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m                     \u001b[0;31m# If we have a view on v, we need to make a copy because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-d5e2124006c8>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Use the InputExample class from BERT's run_classifier code to create examples from the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n\u001b[0m\u001b[1;32m      3\u001b[0m                                                                    \u001b[0mtext_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDATA_COLUMN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                                                    \u001b[0mtext_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                                                    label = x[LABEL_COLUMN]), axis = 1)\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'bert' has no attribute 'run_classifier'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCZWZtKxObjh"
      },
      "source": [
        "Next, we need to preprocess our data so that it matches the data BERT was trained on. For this, we'll need to do a couple of things (but don't worry--this is also included in the Python library):\n",
        "\n",
        "\n",
        "1. Lowercase our text (if we're using a BERT lowercase model)\n",
        "2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\n",
        "3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\n",
        "4. Map our words to indexes using a vocab file that BERT provides\n",
        "5. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\n",
        "6. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\n",
        "\n",
        "Happily, we don't have to worry about most of these details.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMWiDtpyQSoU"
      },
      "source": [
        "To start, we'll need to load a vocabulary file and lowercasing information directly from the BERT tf hub module:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhJSe0QHNG7U"
      },
      "source": [
        "# This is a path to an uncased (all lowercase) version of BERT\n",
        "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
        "\n",
        "def create_tokenizer_from_hub_module():\n",
        "  \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
        "  with tf.Graph().as_default():\n",
        "    bert_module = hub.Module(BERT_MODEL_HUB)\n",
        "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
        "    with tf.Session() as sess:\n",
        "      vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
        "                                            tokenization_info[\"do_lower_case\"]])\n",
        "\n",
        "  return bert.tokenization.FullTokenizer(\n",
        "      vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
        "\n",
        "tokenizer = create_tokenizer_from_hub_module()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4oFkhpZBDKm"
      },
      "source": [
        "Great--we just learned that the BERT model we're using expects lowercase data (that's what stored in tokenization_info[\"do_lower_case\"]) and we also loaded BERT's vocab file. We also created a tokenizer, which breaks words into word pieces:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsBo6RCtQmwx"
      },
      "source": [
        "tokenizer.tokenize(\"This here's an example of using the BERT tokenizer\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OEzfFIt6GIc"
      },
      "source": [
        "Using our tokenizer, we'll call `run_classifier.convert_examples_to_features` on our InputExamples to convert them into features BERT understands."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LL5W8gEGRTAf"
      },
      "source": [
        "# We'll set sequences to be at most 128 tokens long.\n",
        "MAX_SEQ_LENGTH = 128\n",
        "# Convert our train and test features to InputFeatures that BERT understands.\n",
        "train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccp5trMwRtmr"
      },
      "source": [
        "#Creating a model\n",
        "\n",
        "Now that we've prepared our data, let's focus on building a model. `create_model` does just this below. First, it loads the BERT tf hub module again (this time to extract the computation graph). Next, it creates a single new layer that will be trained to adapt BERT to our sentiment task (i.e. classifying whether a movie review is positive or negative). This strategy of using a mostly trained model is called [fine-tuning](http://wiki.fast.ai/index.php/Fine_tuning)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def gelu(x):\n",
        "    \"\"\"Gaussian Error Linear Unit.\n",
        "\n",
        "    This is a smoother version of the RELU.\n",
        "    Original paper: https://arxiv.org/abs/1606.08415\n",
        "    Args:\n",
        "      x: float Tensor to perform activation.\n",
        "\n",
        "    Returns:\n",
        "      `x` with the GELU activation applied.\n",
        "    \"\"\"\n",
        "    cdf = 0.5 * (1.0 + tf.tanh(\n",
        "        (np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3)))))\n",
        "    return x * cdf\n",
        "\n",
        "\n",
        "def get_activation(activation_string):\n",
        "    \"\"\"Maps a string to a Python function, e.g., \"relu\" => `tf.nn.relu`.\n",
        "\n",
        "    Args:\n",
        "      activation_string: String name of the activation function.\n",
        "\n",
        "    Returns:\n",
        "      A Python function corresponding to the activation function. If\n",
        "      `activation_string` is None, empty, or \"linear\", this will return None.\n",
        "      If `activation_string` is not a string, it will return `activation_string`.\n",
        "\n",
        "    Raises:\n",
        "      ValueError: The `activation_string` does not correspond to a known\n",
        "        activation.\n",
        "    \"\"\"\n",
        "\n",
        "    # We assume that anything that\"s not a string is already an activation\n",
        "    # function, so we just return it.\n",
        "    if not isinstance(activation_string, six.string_types):\n",
        "        return activation_string\n",
        "\n",
        "    if not activation_string:\n",
        "        return None\n",
        "\n",
        "    act = activation_string.lower()\n",
        "    if act == \"linear\":\n",
        "        return None\n",
        "    elif act == \"relu\":\n",
        "        return tf.nn.relu\n",
        "    elif act == \"gelu\":\n",
        "        return gelu\n",
        "    elif act == \"tanh\":\n",
        "        return tf.tanh\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported activation: %s\" % act)\n",
        "\n",
        "\n",
        "def get_assignment_map_from_checkpoint(tvars, init_checkpoint):\n",
        "    \"\"\"Compute the union of the current variables and checkpoint variables.\"\"\"\n",
        "    assignment_map = {}\n",
        "    initialized_variable_names = {}\n",
        "\n",
        "    name_to_variable = collections.OrderedDict()\n",
        "    for var in tvars:\n",
        "        name = var.name\n",
        "        m = re.match(\"^(.*):\\\\d+$\", name)\n",
        "        if m is not None:\n",
        "            name = m.group(1)\n",
        "        name_to_variable[name] = var\n",
        "\n",
        "    init_vars = tf.train.list_variables(init_checkpoint)\n",
        "\n",
        "    assignment_map = collections.OrderedDict()\n",
        "    for x in init_vars:\n",
        "        (name, var) = (x[0], x[1])\n",
        "        if name not in name_to_variable:\n",
        "            continue\n",
        "        assignment_map[name] = name\n",
        "        initialized_variable_names[name] = 1\n",
        "        initialized_variable_names[name + \":0\"] = 1\n",
        "\n",
        "    return (assignment_map, initialized_variable_names)\n",
        "\n",
        "\n",
        "def dropout(input_tensor, dropout_prob):\n",
        "    \"\"\"Perform dropout.\n",
        "\n",
        "    Args:\n",
        "      input_tensor: float Tensor.\n",
        "      dropout_prob: Python float. The probability of dropping out a value (NOT of\n",
        "        *keeping* a dimension as in `tf.nn.dropout`).\n",
        "\n",
        "    Returns:\n",
        "      A version of `input_tensor` with dropout applied.\n",
        "    \"\"\"\n",
        "    if dropout_prob is None or dropout_prob == 0.0:\n",
        "        return input_tensor\n",
        "\n",
        "    output = tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n",
        "    return output\n",
        "\n",
        "\n",
        "def layer_norm(input_tensor, name=None):\n",
        "    \"\"\"Run layer normalization on the last dimension of the tensor.\"\"\"\n",
        "def layer_norm(input_tensor, name=None):\n",
        "    \"\"\"Run layer normalization on the last dimension of the tensor.\"\"\"\n",
        "    layer_norma = tf.keras.layers.LayerNormalization(axis = -1)\n",
        "    return layer_norma(input_tensor)\n",
        "\n",
        "    # return tf.compat.v1.contrib.layers.layer_norm(\n",
        "    #     inputs=input_tensor, begin_norm_axis=-1, begin_params_axis=-1, scope=name)\n",
        "\n",
        "\n",
        "def layer_norm_and_dropout(input_tensor, dropout_prob, name=None):\n",
        "    \"\"\"Runs layer normalization followed by dropout.\"\"\"\n",
        "    output_tensor = layer_norm(input_tensor, name)\n",
        "    output_tensor = dropout(output_tensor, dropout_prob)\n",
        "    return output_tensor\n",
        "\n",
        "\n",
        "def create_initializer(initializer_range=0.02):\n",
        "    \"\"\"Creates a `truncated_normal_initializer` with the given range.\"\"\"\n",
        "    return tf.compat.v1.truncated_normal_initializer(stddev=initializer_range)\n",
        "\n",
        "\n",
        "def embedding_lookup(input_ids,\n",
        "                     vocab_size,\n",
        "                     embedding_size=128,\n",
        "                     initializer_range=0.02,\n",
        "                     word_embedding_name=\"word_embeddings\",\n",
        "                     use_one_hot_embeddings=False):\n",
        "    \"\"\"Looks up words embeddings for id tensor.\n",
        "\n",
        "    Args:\n",
        "      input_ids: int32 Tensor of shape [batch_size, seq_length] containing word\n",
        "        ids.\n",
        "      vocab_size: int. Size of the embedding vocabulary.\n",
        "      embedding_size: int. Width of the word embeddings.\n",
        "      initializer_range: float. Embedding initialization range.\n",
        "      word_embedding_name: string. Name of the embedding table.\n",
        "      use_one_hot_embeddings: bool. If True, use one-hot method for word\n",
        "        embeddings. If False, use `tf.gather()`.\n",
        "\n",
        "    Returns:\n",
        "      float Tensor of shape [batch_size, seq_length, embedding_size].\n",
        "    \"\"\"\n",
        "    # This function assumes that the input is of shape [batch_size, seq_length,\n",
        "    # num_inputs].\n",
        "    #\n",
        "    # If the input is a 2D tensor of shape [batch_size, seq_length], we\n",
        "    # reshape to [batch_size, seq_length, 1].\n",
        "    if input_ids.shape.ndims == 2:\n",
        "        input_ids = tf.expand_dims(input_ids, axis=[-1])\n",
        "\n",
        "    embedding_table = tf.compat.v1.get_variable(\n",
        "        name=word_embedding_name,\n",
        "        shape=[vocab_size, embedding_size],\n",
        "        initializer=create_initializer(initializer_range))\n",
        "\n",
        "    flat_input_ids = tf.reshape(input_ids, [-1])\n",
        "    if use_one_hot_embeddings:\n",
        "        one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size)\n",
        "        output = tf.matmul(one_hot_input_ids, embedding_table)\n",
        "    else:\n",
        "        output = tf.gather(embedding_table, flat_input_ids)\n",
        "\n",
        "    input_shape = get_shape_list(input_ids)\n",
        "\n",
        "    output = tf.reshape(output,\n",
        "                        input_shape[0:-1] + [input_shape[-1] * embedding_size])\n",
        "    return (output, embedding_table)\n",
        "\n",
        "\n",
        "def embedding_postprocessor(input_tensor,\n",
        "                            use_token_type=False,\n",
        "                            token_type_ids=None,\n",
        "                            token_type_vocab_size=16,\n",
        "                            token_type_embedding_name=\"token_type_embeddings\",\n",
        "                            use_position_embeddings=True,\n",
        "                            position_embedding_name=\"position_embeddings\",\n",
        "                            initializer_range=0.02,\n",
        "                            max_position_embeddings=512,\n",
        "                            dropout_prob=0.1):\n",
        "    \"\"\"Performs various post-processing on a word embedding tensor.\n",
        "\n",
        "    Args:\n",
        "      input_tensor: float Tensor of shape [batch_size, seq_length,\n",
        "        embedding_size].\n",
        "      use_token_type: bool. Whether to add embeddings for `token_type_ids`.\n",
        "      token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n",
        "        Must be specified if `use_token_type` is True.\n",
        "      token_type_vocab_size: int. The vocabulary size of `token_type_ids`.\n",
        "      token_type_embedding_name: string. The name of the embedding table variable\n",
        "        for token type ids.\n",
        "      use_position_embeddings: bool. Whether to add position embeddings for the\n",
        "        position of each token in the sequence.\n",
        "      position_embedding_name: string. The name of the embedding table variable\n",
        "        for positional embeddings.\n",
        "      initializer_range: float. Range of the weight initialization.\n",
        "      max_position_embeddings: int. Maximum sequence length that might ever be\n",
        "        used with this model. This can be longer than the sequence length of\n",
        "        input_tensor, but cannot be shorter.\n",
        "      dropout_prob: float. Dropout probability applied to the final output tensor.\n",
        "\n",
        "    Returns:\n",
        "      float tensor with same shape as `input_tensor`.\n",
        "\n",
        "    Raises:\n",
        "      ValueError: One of the tensor shapes or input values is invalid.\n",
        "    \"\"\"\n",
        "    input_shape = get_shape_list(input_tensor, expected_rank=3)\n",
        "    batch_size = input_shape[0]\n",
        "    seq_length = input_shape[1]\n",
        "    width = input_shape[2]\n",
        "\n",
        "    output = input_tensor\n",
        "\n",
        "    if use_token_type:\n",
        "        if token_type_ids is None:\n",
        "            raise ValueError(\"`token_type_ids` must be specified if\"\n",
        "                             \"`use_token_type` is True.\")\n",
        "        token_type_table = tf.compat.v1.get_variable(\n",
        "            name=token_type_embedding_name,\n",
        "            shape=[token_type_vocab_size, width],\n",
        "            initializer=create_initializer(initializer_range))\n",
        "        # This vocab will be small so we always do one-hot here, since it is always\n",
        "        # faster for a small vocabulary.\n",
        "        flat_token_type_ids = tf.reshape(token_type_ids, [-1])\n",
        "        one_hot_ids = tf.one_hot(flat_token_type_ids, depth=token_type_vocab_size)\n",
        "        token_type_embeddings = tf.matmul(one_hot_ids, token_type_table)\n",
        "        token_type_embeddings = tf.reshape(token_type_embeddings,\n",
        "                                           [batch_size, seq_length, width])\n",
        "        output += token_type_embeddings\n",
        "\n",
        "    if use_position_embeddings:\n",
        "        assert_op = tf.assert_less_equal(seq_length, max_position_embeddings)\n",
        "        with tf.control_dependencies([assert_op]):\n",
        "            full_position_embeddings = tf.get_variable(\n",
        "                name=position_embedding_name,\n",
        "                shape=[max_position_embeddings, width],\n",
        "                initializer=create_initializer(initializer_range))\n",
        "            # Since the position embedding table is a learned variable, we create it\n",
        "            # using a (long) sequence length `max_position_embeddings`. The actual\n",
        "            # sequence length might be shorter than this, for faster training of\n",
        "            # tasks that do not have long sequences.\n",
        "            #\n",
        "            # So `full_position_embeddings` is effectively an embedding table\n",
        "            # for position [0, 1, 2, ..., max_position_embeddings-1], and the current\n",
        "            # sequence has positions [0, 1, 2, ... seq_length-1], so we can just\n",
        "            # perform a slice.\n",
        "            position_embeddings = tf.slice(full_position_embeddings, [0, 0],\n",
        "                                           [seq_length, -1])\n",
        "            num_dims = len(output.shape.as_list())\n",
        "\n",
        "            # Only the last two dimensions are relevant (`seq_length` and `width`), so\n",
        "            # we broadcast among the first dimensions, which is typically just\n",
        "            # the batch size.\n",
        "            position_broadcast_shape = []\n",
        "            for _ in range(num_dims - 2):\n",
        "                position_broadcast_shape.append(1)\n",
        "            position_broadcast_shape.extend([seq_length, width])\n",
        "            position_embeddings = tf.reshape(position_embeddings,\n",
        "                                             position_broadcast_shape)\n",
        "            output += position_embeddings\n",
        "\n",
        "    output = layer_norm_and_dropout(output, dropout_prob)\n",
        "    return output\n",
        "\n",
        "\n",
        "def create_attention_mask_from_input_mask(from_tensor, to_mask):\n",
        "    \"\"\"Create 3D attention mask from a 2D tensor mask.\n",
        "\n",
        "    Args:\n",
        "      from_tensor: 2D or 3D Tensor of shape [batch_size, from_seq_length, ...].\n",
        "      to_mask: int32 Tensor of shape [batch_size, to_seq_length].\n",
        "\n",
        "    Returns:\n",
        "      float Tensor of shape [batch_size, from_seq_length, to_seq_length].\n",
        "    \"\"\"\n",
        "    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n",
        "    batch_size = from_shape[0]\n",
        "    from_seq_length = from_shape[1]\n",
        "\n",
        "    to_shape = get_shape_list(to_mask, expected_rank=2)\n",
        "    to_seq_length = to_shape[1]\n",
        "\n",
        "    to_mask = tf.cast(\n",
        "        tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n",
        "\n",
        "    # We don't assume that `from_tensor` is a mask (although it could be). We\n",
        "    # don't actually care if we attend *from* padding tokens (only *to* padding)\n",
        "    # tokens so we create a tensor of all ones.\n",
        "    #\n",
        "    # `broadcast_ones` = [batch_size, from_seq_length, 1]\n",
        "    broadcast_ones = tf.ones(\n",
        "        shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n",
        "\n",
        "    # Here we broadcast along two dimensions to create the mask.\n",
        "    mask = broadcast_ones * to_mask\n",
        "\n",
        "    return mask\n",
        "\n",
        "\n",
        "def attention_layer(from_tensor,\n",
        "                    to_tensor,\n",
        "                    attention_mask=None,\n",
        "                    num_attention_heads=1,\n",
        "                    size_per_head=512,\n",
        "                    query_act=None,\n",
        "                    key_act=None,\n",
        "                    value_act=None,\n",
        "                    attention_probs_dropout_prob=0.0,\n",
        "                    initializer_range=0.02,\n",
        "                    do_return_2d_tensor=False,\n",
        "                    batch_size=None,\n",
        "                    from_seq_length=None,\n",
        "                    to_seq_length=None):\n",
        "    \"\"\"Performs multi-headed attention from `from_tensor` to `to_tensor`.\n",
        "\n",
        "    This is an implementation of multi-headed attention based on \"Attention\n",
        "    is all you Need\". If `from_tensor` and `to_tensor` are the same, then\n",
        "    this is self-attention. Each timestep in `from_tensor` attends to the\n",
        "    corresponding sequence in `to_tensor`, and returns a fixed-with vector.\n",
        "\n",
        "    This function first projects `from_tensor` into a \"query\" tensor and\n",
        "    `to_tensor` into \"key\" and \"value\" tensors. These are (effectively) a list\n",
        "    of tensors of length `num_attention_heads`, where each tensor is of shape\n",
        "    [batch_size, seq_length, size_per_head].\n",
        "\n",
        "    Then, the query and key tensors are dot-producted and scaled. These are\n",
        "    softmaxed to obtain attention probabilities. The value tensors are then\n",
        "    interpolated by these probabilities, then concatenated back to a single\n",
        "    tensor and returned.\n",
        "\n",
        "    In practice, the multi-headed attention are done with transposes and\n",
        "    reshapes rather than actual separate tensors.\n",
        "\n",
        "    Args:\n",
        "      from_tensor: float Tensor of shape [batch_size, from_seq_length,\n",
        "        from_width].\n",
        "      to_tensor: float Tensor of shape [batch_size, to_seq_length, to_width].\n",
        "      attention_mask: (optional) int32 Tensor of shape [batch_size,\n",
        "        from_seq_length, to_seq_length]. The values should be 1 or 0. The\n",
        "        attention scores will effectively be set to -infinity for any positions in\n",
        "        the mask that are 0, and will be unchanged for positions that are 1.\n",
        "      num_attention_heads: int. Number of attention heads.\n",
        "      size_per_head: int. Size of each attention head.\n",
        "      query_act: (optional) Activation function for the query transform.\n",
        "      key_act: (optional) Activation function for the key transform.\n",
        "      value_act: (optional) Activation function for the value transform.\n",
        "      attention_probs_dropout_prob: (optional) float. Dropout probability of the\n",
        "        attention probabilities.\n",
        "      initializer_range: float. Range of the weight initializer.\n",
        "      do_return_2d_tensor: bool. If True, the output will be of shape [batch_size\n",
        "        * from_seq_length, num_attention_heads * size_per_head]. If False, the\n",
        "        output will be of shape [batch_size, from_seq_length, num_attention_heads\n",
        "        * size_per_head].\n",
        "      batch_size: (Optional) int. If the input is 2D, this might be the batch size\n",
        "        of the 3D version of the `from_tensor` and `to_tensor`.\n",
        "      from_seq_length: (Optional) If the input is 2D, this might be the seq length\n",
        "        of the 3D version of the `from_tensor`.\n",
        "      to_seq_length: (Optional) If the input is 2D, this might be the seq length\n",
        "        of the 3D version of the `to_tensor`.\n",
        "\n",
        "    Returns:\n",
        "      float Tensor of shape [batch_size, from_seq_length,\n",
        "        num_attention_heads * size_per_head]. (If `do_return_2d_tensor` is\n",
        "        true, this will be of shape [batch_size * from_seq_length,\n",
        "        num_attention_heads * size_per_head]).\n",
        "\n",
        "    Raises:\n",
        "      ValueError: Any of the arguments or tensor shapes are invalid.\n",
        "    \"\"\"\n",
        "\n",
        "    def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n",
        "                             seq_length, width):\n",
        "        output_tensor = tf.reshape(\n",
        "            input_tensor, [batch_size, seq_length, num_attention_heads, width])\n",
        "\n",
        "        output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n",
        "        return output_tensor\n",
        "\n",
        "    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n",
        "    to_shape = get_shape_list(to_tensor, expected_rank=[2, 3])\n",
        "\n",
        "    if len(from_shape) != len(to_shape):\n",
        "        raise ValueError(\n",
        "            \"The rank of `from_tensor` must match the rank of `to_tensor`.\")\n",
        "\n",
        "    if len(from_shape) == 3:\n",
        "        batch_size = from_shape[0]\n",
        "        from_seq_length = from_shape[1]\n",
        "        to_seq_length = to_shape[1]\n",
        "    elif len(from_shape) == 2:\n",
        "        if (batch_size is None or from_seq_length is None or to_seq_length is None):\n",
        "            raise ValueError(\n",
        "                \"When passing in rank 2 tensors to attention_layer, the values \"\n",
        "                \"for `batch_size`, `from_seq_length`, and `to_seq_length` \"\n",
        "                \"must all be specified.\")\n",
        "\n",
        "    # Scalar dimensions referenced here:\n",
        "    #   B = batch size (number of sequences)\n",
        "    #   F = `from_tensor` sequence length\n",
        "    #   T = `to_tensor` sequence length\n",
        "    #   N = `num_attention_heads`\n",
        "    #   H = `size_per_head`\n",
        "\n",
        "    from_tensor_2d = reshape_to_matrix(from_tensor)\n",
        "    to_tensor_2d = reshape_to_matrix(to_tensor)\n",
        "\n",
        "    # `query_layer` = [B*F, N*H]\n",
        "    query_layer = tf.keras.layers.Dense(num_attention_heads * size_per_head, name='query')(from_tensor_2d)\n",
        "    # query_layer = tf.layers.dense(\n",
        "    #     from_tensor_2d,\n",
        "    #     num_attention_heads * size_per_head,\n",
        "    #     activation=query_act,\n",
        "    #     name=\"query\",\n",
        "    #     kernel_initializer=create_initializer(initializer_range))\n",
        "\n",
        "    # `key_layer` = [B*T, N*H]\n",
        "    key_layer = tf.keras.layers.Dense(num_attention_heads * size_per_head, name='key')(from_tensor_2d)\n",
        "    # key_layer = tf.layers.dense(\n",
        "    #     to_tensor_2d,\n",
        "    #     num_attention_heads * size_per_head,\n",
        "    #     activation=key_act,\n",
        "    #     name=\"key\",\n",
        "    #     kernel_initializer=create_initializer(initializer_range))\n",
        "\n",
        "    # `value_layer` = [B*T, N*H]\n",
        "    value_layer = tf.keras.layers.Dense(num_attention_heads * size_per_head, name='value')(from_tensor_2d)\n",
        "    # value_layer = tf.layers.dense(\n",
        "    #     to_tensor_2d,\n",
        "    #     num_attention_heads * size_per_head,\n",
        "    #     activation=value_act,\n",
        "    #     name=\"value\",\n",
        "    #     kernel_initializer=create_initializer(initializer_range))\n",
        "\n",
        "    # `query_layer` = [B, N, F, H]\n",
        "    query_layer = transpose_for_scores(query_layer, batch_size,\n",
        "                                       num_attention_heads, from_seq_length,\n",
        "                                       size_per_head)\n",
        "\n",
        "    # `key_layer` = [B, N, T, H]\n",
        "    key_layer = transpose_for_scores(key_layer, batch_size, num_attention_heads,\n",
        "                                     to_seq_length, size_per_head)\n",
        "\n",
        "    # Take the dot product between \"query\" and \"key\" to get the raw\n",
        "    # attention scores.\n",
        "    # `attention_scores` = [B, N, F, T]\n",
        "    attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)\n",
        "    attention_scores = tf.multiply(attention_scores,\n",
        "                                   1.0 / math.sqrt(float(size_per_head)))\n",
        "\n",
        "    if attention_mask is not None:\n",
        "        # `attention_mask` = [B, 1, F, T]\n",
        "        attention_mask = tf.expand_dims(attention_mask, axis=[1])\n",
        "\n",
        "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
        "        # masked positions, this operation will create a tensor which is 0.0 for\n",
        "        # positions we want to attend and -10000.0 for masked positions.\n",
        "        adder = (1.0 - tf.cast(attention_mask, tf.float32)) * -10000.0\n",
        "\n",
        "        # Since we are adding it to the raw scores before the softmax, this is\n",
        "        # effectively the same as removing these entirely.\n",
        "        attention_scores += adder\n",
        "\n",
        "    # Normalize the attention scores to probabilities.\n",
        "    # `attention_probs` = [B, N, F, T]\n",
        "    attention_probs = tf.nn.softmax(attention_scores)\n",
        "\n",
        "    # This is actually dropping out entire tokens to attend to, which might\n",
        "    # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "    attention_probs = dropout(attention_probs, attention_probs_dropout_prob)\n",
        "\n",
        "    # `value_layer` = [B, T, N, H]\n",
        "    value_layer = tf.reshape(\n",
        "        value_layer,\n",
        "        [batch_size, to_seq_length, num_attention_heads, size_per_head])\n",
        "\n",
        "    # `value_layer` = [B, N, T, H]\n",
        "    value_layer = tf.transpose(value_layer, [0, 2, 1, 3])\n",
        "\n",
        "    # `context_layer` = [B, N, F, H]\n",
        "    context_layer = tf.matmul(attention_probs, value_layer)\n",
        "\n",
        "    # `context_layer` = [B, F, N, H]\n",
        "    context_layer = tf.transpose(context_layer, [0, 2, 1, 3])\n",
        "\n",
        "    if do_return_2d_tensor:\n",
        "        # `context_layer` = [B*F, N*H]\n",
        "        context_layer = tf.reshape(\n",
        "            context_layer,\n",
        "            [batch_size * from_seq_length, num_attention_heads * size_per_head])\n",
        "    else:\n",
        "        # `context_layer` = [B, F, N*H]\n",
        "        context_layer = tf.reshape(\n",
        "            context_layer,\n",
        "            [batch_size, from_seq_length, num_attention_heads * size_per_head])\n",
        "\n",
        "    return context_layer\n",
        "\n",
        "\n",
        "def transformer_model(input_tensor,\n",
        "                      attention_mask=None,\n",
        "                      hidden_size=768,\n",
        "                      num_hidden_layers=12,\n",
        "                      num_attention_heads=12,\n",
        "                      intermediate_size=3072,\n",
        "                      intermediate_act_fn=gelu,\n",
        "                      hidden_dropout_prob=0.1,\n",
        "                      attention_probs_dropout_prob=0.1,\n",
        "                      initializer_range=0.02,\n",
        "                      do_return_all_layers=False):\n",
        "    \"\"\"Multi-headed, multi-layer Transformer from \"Attention is All You Need\".\n",
        "\n",
        "    This is almost an exact implementation of the original Transformer encoder.\n",
        "\n",
        "    See the original paper:\n",
        "    https://arxiv.org/abs/1706.03762\n",
        "\n",
        "    Also see:\n",
        "    https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py\n",
        "\n",
        "    Args:\n",
        "      input_tensor: float Tensor of shape [batch_size, seq_length, hidden_size].\n",
        "      attention_mask: (optional) int32 Tensor of shape [batch_size, seq_length,\n",
        "        seq_length], with 1 for positions that can be attended to and 0 in\n",
        "        positions that should not be.\n",
        "      hidden_size: int. Hidden size of the Transformer.\n",
        "      num_hidden_layers: int. Number of layers (blocks) in the Transformer.\n",
        "      num_attention_heads: int. Number of attention heads in the Transformer.\n",
        "      intermediate_size: int. The size of the \"intermediate\" (a.k.a., feed\n",
        "        forward) layer.\n",
        "      intermediate_act_fn: function. The non-linear activation function to apply\n",
        "        to the output of the intermediate/feed-forward layer.\n",
        "      hidden_dropout_prob: float. Dropout probability for the hidden layers.\n",
        "      attention_probs_dropout_prob: float. Dropout probability of the attention\n",
        "        probabilities.\n",
        "      initializer_range: float. Range of the initializer (stddev of truncated\n",
        "        normal).\n",
        "      do_return_all_layers: Whether to also return all layers or just the final\n",
        "        layer.\n",
        "\n",
        "    Returns:\n",
        "      float Tensor of shape [batch_size, seq_length, hidden_size], the final\n",
        "      hidden layer of the Transformer.\n",
        "\n",
        "    Raises:\n",
        "      ValueError: A Tensor shape or parameter is invalid.\n",
        "    \"\"\"\n",
        "    if hidden_size % num_attention_heads != 0:\n",
        "        raise ValueError(\n",
        "            \"The hidden size (%d) is not a multiple of the number of attention \"\n",
        "            \"heads (%d)\" % (hidden_size, num_attention_heads))\n",
        "\n",
        "    attention_head_size = int(hidden_size / num_attention_heads)\n",
        "    input_shape = get_shape_list(input_tensor, expected_rank=3)\n",
        "    batch_size = input_shape[0]\n",
        "    seq_length = input_shape[1]\n",
        "    input_width = input_shape[2]\n",
        "\n",
        "    # The Transformer performs sum residuals on all layers so the input needs\n",
        "    # to be the same as the hidden size.\n",
        "    if input_width != hidden_size:\n",
        "        raise ValueError(\"The width of the input tensor (%d) != hidden size (%d)\" %\n",
        "                         (input_width, hidden_size))\n",
        "\n",
        "    # We keep the representation as a 2D tensor to avoid re-shaping it back and\n",
        "    # forth from a 3D tensor to a 2D tensor. Re-shapes are normally free on\n",
        "    # the GPU/CPU but may not be free on the TPU, so we want to minimize them to\n",
        "    # help the optimizer.\n",
        "    prev_output = reshape_to_matrix(input_tensor)\n",
        "\n",
        "    all_layer_outputs = []\n",
        "    for layer_idx in range(num_hidden_layers):\n",
        "        with tf.compat.v1.variable_scope(\"layer_%d\" % layer_idx):\n",
        "            layer_input = prev_output\n",
        "\n",
        "            with tf.compat.v1.variable_scope(\"attention\"):\n",
        "                attention_heads = []\n",
        "                with tf.compat.v1.variable_scope(\"self\"):\n",
        "                    attention_head = attention_layer(\n",
        "                        from_tensor=layer_input,\n",
        "                        to_tensor=layer_input,\n",
        "                        attention_mask=attention_mask,\n",
        "                        num_attention_heads=num_attention_heads,\n",
        "                        size_per_head=attention_head_size,\n",
        "                        attention_probs_dropout_prob=attention_probs_dropout_prob,\n",
        "                        initializer_range=initializer_range,\n",
        "                        do_return_2d_tensor=True,\n",
        "                        batch_size=batch_size,\n",
        "                        from_seq_length=seq_length,\n",
        "                        to_seq_length=seq_length)\n",
        "                    attention_heads.append(attention_head)\n",
        "\n",
        "                attention_output = None\n",
        "                if len(attention_heads) == 1:\n",
        "                    attention_output = attention_heads[0]\n",
        "                else:\n",
        "                    # In the case where we have other sequences, we just concatenate\n",
        "                    # them to the self-attention head before the projection.\n",
        "                    attention_output = tf.concat(attention_heads, axis=-1)\n",
        "\n",
        "                # Run a linear projection of `hidden_size` then add a residual\n",
        "                # with `layer_input`.\n",
        "                with tf.compat.v1.variable_scope(\"output\"):\n",
        "                    attention_output = tf.keras.layers.Dense(hidden_size)(attention_output)\n",
        "                    # attention_output = tf.layers.dense(\n",
        "                    #     attention_output,\n",
        "                    #     hidden_size,\n",
        "                    #     kernel_initializer=create_initializer(initializer_range))\n",
        "                    attention_output = dropout(attention_output, hidden_dropout_prob)\n",
        "                    attention_output = layer_norm(attention_output + layer_input)\n",
        "\n",
        "            # The activation is only applied to the \"intermediate\" hidden layer.\n",
        "            with tf.compat.v1.variable_scope(\"intermediate\"):\n",
        "                intermediate_output = tf.keras.layers.Dense(intermediate_size)(attention_output)\n",
        "                # intermediate_output = tf.layers.dense(\n",
        "                #     attention_output,\n",
        "                #     intermediate_size,\n",
        "                #     activation=intermediate_act_fn,\n",
        "                #     kernel_initializer=create_initializer(initializer_range))\n",
        "\n",
        "            # Down-project back to `hidden_size` then add the residual.\n",
        "            with tf.compat.v1.variable_scope(\"output\"):\n",
        "                layer_output = tf.keras.layers.Dense(hidden_size)(intermediate_output)\n",
        "                # layer_output = tf.layers.dense(\n",
        "                #     intermediate_output,\n",
        "                #     hidden_size,\n",
        "                #     kernel_initializer=create_initializer(initializer_range))\n",
        "                layer_output = dropout(layer_output, hidden_dropout_prob)\n",
        "                layer_output = layer_norm(layer_output + attention_output)\n",
        "                prev_output = layer_output\n",
        "                all_layer_outputs.append(layer_output)\n",
        "\n",
        "    if do_return_all_layers:\n",
        "        final_outputs = []\n",
        "        for layer_output in all_layer_outputs:\n",
        "            final_output = reshape_from_matrix(layer_output, input_shape)\n",
        "            final_outputs.append(final_output)\n",
        "        return final_outputs\n",
        "    else:\n",
        "        final_output = reshape_from_matrix(prev_output, input_shape)\n",
        "        return final_output\n",
        "\n",
        "\n",
        "def get_shape_list(tensor, expected_rank=None, name=None):\n",
        "    \"\"\"Returns a list of the shape of tensor, preferring static dimensions.\n",
        "\n",
        "    Args:\n",
        "      tensor: A tf.Tensor object to find the shape of.\n",
        "      expected_rank: (optional) int. The expected rank of `tensor`. If this is\n",
        "        specified and the `tensor` has a different rank, and exception will be\n",
        "        thrown.\n",
        "      name: Optional name of the tensor for the error message.\n",
        "\n",
        "    Returns:\n",
        "      A list of dimensions of the shape of tensor. All static dimensions will\n",
        "      be returned as python integers, and dynamic dimensions will be returned\n",
        "      as tf.Tensor scalars.\n",
        "    \"\"\"\n",
        "    if name is None:\n",
        "        name = tensor.name\n",
        "\n",
        "    if expected_rank is not None:\n",
        "        assert_rank(tensor, expected_rank, name)\n",
        "\n",
        "    shape = tensor.shape.as_list()\n",
        "\n",
        "    non_static_indexes = []\n",
        "    for (index, dim) in enumerate(shape):\n",
        "        if dim is None:\n",
        "            non_static_indexes.append(index)\n",
        "\n",
        "    if not non_static_indexes:\n",
        "        return shape\n",
        "\n",
        "    dyn_shape = tf.shape(tensor)\n",
        "    for index in non_static_indexes:\n",
        "        shape[index] = dyn_shape[index]\n",
        "    return shape\n",
        "\n",
        "\n",
        "def reshape_to_matrix(input_tensor):\n",
        "    \"\"\"Reshapes a >= rank 2 tensor to a rank 2 tensor (i.e., a matrix).\"\"\"\n",
        "    ndims = input_tensor.shape.ndims\n",
        "    if ndims < 2:\n",
        "        raise ValueError(\"Input tensor must have at least rank 2. Shape = %s\" %\n",
        "                         (input_tensor.shape))\n",
        "    if ndims == 2:\n",
        "        return input_tensor\n",
        "\n",
        "    width = input_tensor.shape[-1]\n",
        "    output_tensor = tf.reshape(input_tensor, [-1, width])\n",
        "    return output_tensor\n",
        "\n",
        "\n",
        "def reshape_from_matrix(output_tensor, orig_shape_list):\n",
        "    \"\"\"Reshapes a rank 2 tensor back to its original rank >= 2 tensor.\"\"\"\n",
        "    if len(orig_shape_list) == 2:\n",
        "        return output_tensor\n",
        "\n",
        "    output_shape = get_shape_list(output_tensor)\n",
        "\n",
        "    orig_dims = orig_shape_list[0:-1]\n",
        "    width = output_shape[-1]\n",
        "\n",
        "    return tf.reshape(output_tensor, orig_dims + [width])\n",
        "\n",
        "\n",
        "def assert_rank(tensor, expected_rank, name=None):\n",
        "    \"\"\"Raises an exception if the tensor rank is not of the expected rank.\n",
        "\n",
        "    Args:\n",
        "      tensor: A tf.Tensor to check the rank of.\n",
        "      expected_rank: Python integer or list of integers, expected rank.\n",
        "      name: Optional name of the tensor for the error message.\n",
        "\n",
        "    Raises:\n",
        "      ValueError: If the expected shape doesn't match the actual shape.\n",
        "    \"\"\"\n",
        "    if name is None:\n",
        "        name = tensor.name\n",
        "\n",
        "    expected_rank_dict = {}\n",
        "    if isinstance(expected_rank, six.integer_types):\n",
        "        expected_rank_dict[expected_rank] = True\n",
        "    else:\n",
        "        for x in expected_rank:\n",
        "            expected_rank_dict[x] = True\n",
        "\n",
        "    actual_rank = tensor.shape.ndims\n",
        "    if actual_rank not in expected_rank_dict:\n",
        "        scope_name = tf.get_variable_scope().name\n",
        "        raise ValueError(\n",
        "            \"For the tensor `%s` in scope `%s`, the actual rank \"\n",
        "            \"`%d` (shape = %s) is not equal to the expected rank `%s`\" %\n",
        "            (name, scope_name, actual_rank, str(tensor.shape), str(expected_rank)))\n"
      ],
      "metadata": {
        "id": "blM0-uviMQMm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import six"
      ],
      "metadata": {
        "id": "VOnuX_4GOjMR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tf.constant([[31, 51, 99], [15, 5, 0]])\n",
        "input_mask = tf.constant([[1, 1, 1], [1, 1, 0]])\n",
        "token_type_ids = tf.constant([[0, 0, 1], [0, 2, 0]])\n",
        "\n",
        "config = modeling.BertConfig(vocab_size=32000, hidden_size=512, num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "scope = None\n",
        "\n",
        "with tf.compat.v1.variable_scope(scope, default_name=\"bert\"):\n",
        "    with tf.compat.v1.variable_scope(\"embeddings\"):\n",
        "        # Perform embedding lookup on the word ids.\n",
        "        (embedding_output, embedding_table) = embedding_lookup(\n",
        "            input_ids=input_ids,\n",
        "            vocab_size=config.vocab_size,\n",
        "            embedding_size=config.hidden_size,\n",
        "            initializer_range=config.initializer_range,\n",
        "            word_embedding_name=\"word_embeddings\",\n",
        "            use_one_hot_embeddings=False)\n",
        "        print(embedding_output)\n",
        "\n",
        "        # Add positional embeddings and token type embeddings, then layer\n",
        "        # normalize and perform dropout.\n",
        "        embedding_output = embedding_postprocessor(\n",
        "            input_tensor=embedding_output,\n",
        "            use_token_type=True,\n",
        "            token_type_ids=token_type_ids,\n",
        "            token_type_embedding_name=\"token_type_embeddings\",\n",
        "            use_position_embeddings=False,\n",
        "            position_embedding_name=\"position_embeddings\",\n",
        "            initializer_range=config.initializer_range,\n",
        "            max_position_embeddings=0.6,\n",
        "            dropout_prob=0.1)\n",
        "        print(embedding_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGjEc2N6KRf6",
        "outputId": "144b0cb9-4dfc-4f51-83f9-1d6eac61e2de"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor(\"bert_1/embeddings/Reshape_1:0\", shape=(2, 3, 512), dtype=float32)\n",
            "Tensor(\"bert_1/embeddings/dropout/SelectV2:0\", shape=(2, 3, 512), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cat_l1_idx = tf.constant([[0], [2]])\n",
        "cat_l2_idx = tf.constant([[10], [20]])\n",
        "with tf.compat.v1.variable_scope(\"embeddings\", reuse=tf.compat.v1.AUTO_REUSE):\n",
        "  l1_dim = 4\n",
        "  cat_l1_table = tf.compat.v1.get_variable(\n",
        "      name='cat_l1_idx_emb',\n",
        "      shape=[290, l1_dim])\n",
        "  l2_dim = 8\n",
        "  cat_l2_table = tf.compat.v1.get_variable(\n",
        "      name='cat_l2_idx_emb',\n",
        "      shape=[42840, l2_dim])\n",
        "\n",
        "  flat_cat_l1_idx = tf.reshape(cat_l1_idx, [-1])\n",
        "  cat_l1_output = tf.gather(cat_l1_table, flat_cat_l1_idx)\n",
        "  cat_l1_shape = modeling.get_shape_list(cat_l1_idx)\n",
        "  cat_l1_output = tf.reshape(cat_l1_output, cat_l1_shape[0:-1] + [cat_l1_shape[-1] * l1_dim])\n",
        "\n",
        "  flat_cat_l2_idx = tf.reshape(cat_l2_idx, [-1])\n",
        "  cat_l2_output = tf.gather(cat_l2_table, flat_cat_l2_idx)\n",
        "  cat_l2_shape = modeling.get_shape_list(cat_l2_idx)\n",
        "  cat_l2_output = tf.reshape(cat_l2_output, cat_l2_shape[0:-1] + [cat_l2_shape[-1] * l2_dim])\n",
        "\n",
        "  print(\"cat_l1_output.shape:\", cat_l1_output.shape) #1, 3*4\n",
        "  print(\"cat_l2_output.shape:\", cat_l2_output.shape) #1, 3*8\n",
        "\n",
        "  embedding_output = tf.keras.layers.Dense(units=80, activation=tf.tanh)(embedding_output)\n",
        "  print(\"embedding_output.shape:\", embedding_output.shape) #batch, seq_length, emb\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6pZ2zFoRR_z",
        "outputId": "92ce368e-bdb1-40e6-a897-2ad56830870e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat_l1_output.shape: (2, 4)\n",
            "cat_l2_output.shape: (2, 8)\n",
            "embedding_output.shape: (2, 3, 80)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "# 创建一个模拟的标签张量，这里用的是一个固定大小的例子，实际应用中可能是动态变化的\n",
        "label = tf.constant([[0], [1]], dtype=tf.int32)  # 形状: (2, 1)\n",
        "with tf.compat.v1.variable_scope(\"encoder\"):\n",
        "    # mask of shape [batch_size, seq_length, seq_length] which is used\n",
        "    attention_mask = create_attention_mask_from_input_mask(\n",
        "        input_ids, input_mask)\n",
        "\n",
        "    # `sequence_output` shape = [batch_size, seq_length, hidden_size].\n",
        "    encoder_layers = transformer_model(\n",
        "        input_tensor=embedding_output,\n",
        "        attention_mask=attention_mask,\n",
        "        hidden_size=80,\n",
        "        num_hidden_layers=2,\n",
        "        num_attention_heads=2,\n",
        "        intermediate_size=164,\n",
        "        intermediate_act_fn=get_activation('gelu'),\n",
        "        initializer_range=0.02,\n",
        "        do_return_all_layers=True)\n",
        "\n",
        "    assert len(encoder_layers) > 1\n",
        "    encoder_pooled_output = tf.reduce_mean((encoder_layers[0] + encoder_layers[-1]) / 2, axis=1)\n",
        "print(\"encoder_layers[0].shape:\", encoder_layers[0].shape)\n",
        "print(\"encoder_layers[1].shape:\", encoder_layers[1].shape)\n",
        "print(\"encoder_pooled_output.shape:\", encoder_pooled_output.shape)\n",
        "item_pooled_output, query_pooled_output = tf.split(encoder_pooled_output, 2, axis=0)\n",
        "\n",
        "query_pooled_output_l2 = tf.math.l2_normalize(query_pooled_output, axis=1)\n",
        "item_pooled_output_l2 = tf.math.l2_normalize(item_pooled_output, axis=1)\n",
        "\n",
        "print(\"query_pooled_output_l2.shape:\", query_pooled_output.shape)\n",
        "\n",
        "print(\"item_pooled_output_l2.shape:\", item_pooled_output.shape)\n",
        "\n",
        "psim = tf.reduce_sum(query_pooled_output_l2 * item_pooled_output_l2, axis=1)\n",
        "print(\"psim.shape:\", psim.shape)\n",
        "\n",
        "psim_expand = tf.stop_gradient(tf.expand_dims(psim, axis=1))\n",
        "psim_expand = tf.where(psim_expand < 0, tf.zeros_like(psim_expand), psim_expand)\n",
        "psim_expand = tf.where(psim_expand > 1, tf.ones_like(psim_expand), psim_expand)\n",
        "print(\"psim_expand.shape:\", psim_expand.shape)\n",
        "print(\"cat_l1_output.shape:\", cat_l1_output.shape)\n"
      ],
      "metadata": {
        "id": "squ-aUFFU2_E",
        "outputId": "7a861f44-157a-4dc0-919d-ac1417ff40df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoder_layers[0].shape: (2, 3, 80)\n",
            "encoder_layers[1].shape: (2, 3, 80)\n",
            "encoder_pooled_output.shape: (2, 80)\n",
            "query_pooled_output_l2.shape: (1, 80)\n",
            "item_pooled_output_l2.shape: (1, 80)\n",
            "psim.shape: (1,)\n",
            "psim_expand.shape: (1, 1)\n",
            "cat_l1_output.shape: (2, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "combined = tf.concat([query_pooled_output, item_pooled_output,\n",
        "                      tf.abs(query_pooled_output - item_pooled_output),\n",
        "                      tf.maximum(query_pooled_output, item_pooled_output),\n",
        "                      # cat_l1_output,\n",
        "                      # cat_l2_output,\n",
        "                      psim_expand\n",
        "                      ], axis=1)\n",
        "prel_wide = tf.keras.layers.Dense(units=1, activation='sigmoid')(combined)\n",
        "gate = tf.keras.layers.Dense(units=1, activation='sigmoid')(combined)\n",
        "prel = gate * psim_expand + (1 - gate) * prel_wide\n",
        "\n",
        "\n",
        "print(\"query_pooled_output.shape:\", query_pooled_output.shape)\n",
        "print(\"combined.shape:\", combined.shape)\n",
        "print(\"item_pooled_output.shape:\", item_pooled_output.shape)\n",
        "print(\"prel.shape:\", prel.shape)\n",
        "print(\"prel_wide.shape:\", prel_wide.shape)\n",
        "print(\"gate.shape:\", gate.shape)\n",
        "print(\"label.shape:\", label.shape)\n",
        "log_loss_wide = tf.compat.v1.losses.log_loss(label, prel_wide)\n",
        "# if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "#     print(\"-\" * 10, tf.trainable_variables())\n",
        "#     label = tf.cast(labels[\"s_label\"], tf.float32)\n",
        "#     log_loss = tf.losses.log_loss(label, prel)\n",
        "#     log_loss_wide = tf.losses.log_loss(label, prel_wide)\n",
        "\n",
        "#     print(\"label.shape:\", label.shape)\n",
        "\n",
        "#     # similarities = tf.matmul(query_pooled_output, item_pooled_output, transpose_b=True)\n",
        "#     # similarities = similarities / params[\"simcse_temp\"]\n",
        "#     # diag_label = tf.linalg.diag(tf.squeeze(label, axis=[1]))\n",
        "#     # simcse_loss = tf.reduce_mean(\n",
        "#     #     tf.keras.losses.categorical_crossentropy(diag_label, similarities, from_logits=True))\n",
        "#     # print(\"similarities.shape:\", similarities.shape)\n",
        "#     # print(\"diag_label.shape:\", diag_label.shape)\n",
        "\n",
        "y_true = tf.squeeze(label, axis=[1])\n",
        "print(\"y_true.shape:\", y_true.shape)\n",
        "y_true = tf.cast(y_true[:, None] < y_true[None, :], tf.keras.backend.floatx())\n",
        "print(\"y_true.shape:\", y_true.shape)\n",
        "similarities = tf.reduce_sum(query_pooled_output_l2 * item_pooled_output_l2, axis=1) / 0.05\n",
        "similarities = similarities[:, None] - similarities[None, :]\n",
        "\n",
        "print(\"similarities.shape:\", similarities.shape)\n",
        "print(\"y_true.shape:\", y_true.shape)\n",
        "\n",
        "similarities = tf.reshape(similarities - (1 - y_true) * 1e12, [-1])\n",
        "similarities = tf.concat([[0], similarities], axis=0)\n",
        "cosent_loss = tf.math.reduce_logsumexp(similarities)\n",
        "\n",
        "#     print(\"similarities1.shape:\", similarities.shape)\n",
        "#     print(\"cosent_loss.shape:\", cosent_loss.shape)\n",
        "\n",
        "#     # loss = log_loss + 0.1 * simcse_loss\n",
        "#     # loss = log_loss + 0.05 * cosent_loss\n",
        "#     # loss = log_loss + 0.09 * cosent_loss\n",
        "#     loss = log_loss + 0.1 * cosent_loss + 0.1 * log_loss_wide"
      ],
      "metadata": {
        "id": "Ic6j6NoY4tRE",
        "outputId": "8f5869b3-90e3-40e0-f2e9-5d49c62b3a7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "query_pooled_output.shape: (1, 80)\n",
            "combined.shape: (1, 321)\n",
            "item_pooled_output.shape: (1, 80)\n",
            "prel.shape: (1, 1)\n",
            "prel_wide.shape: (1, 1)\n",
            "gate.shape: (1, 1)\n",
            "label.shape: (2, 1)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Shapes (1, 1) and (2, 1) are incompatible",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-b86a670e234c>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gate.shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"label.shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mlog_loss_wide\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprel_wide\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;31m# if mode == tf.estimator.ModeKeys.TRAIN:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m#     print(\"-\" * 10, tf.trainable_variables())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36massert_is_compatible_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     \"\"\"\n\u001b[1;32m   1383\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shapes %s and %s are incompatible\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mmost_specific_compatible_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"TensorShape\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Shapes (1, 1) and (2, 1) are incompatible"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6o2a5ZIvRcJq"
      },
      "source": [
        "def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,\n",
        "                 num_labels):\n",
        "  \"\"\"Creates a classification model.\"\"\"\n",
        "\n",
        "  bert_module = hub.Module(\n",
        "      BERT_MODEL_HUB,\n",
        "      trainable=True)\n",
        "  bert_inputs = dict(\n",
        "      input_ids=input_ids,\n",
        "      input_mask=input_mask,\n",
        "      segment_ids=segment_ids)\n",
        "  bert_outputs = bert_module(\n",
        "      inputs=bert_inputs,\n",
        "      signature=\"tokens\",\n",
        "      as_dict=True)\n",
        "\n",
        "  # Use \"pooled_output\" for classification tasks on an entire sentence.\n",
        "  # Use \"sequence_outputs\" for token-level output.\n",
        "  output_layer = bert_outputs[\"pooled_output\"]\n",
        "\n",
        "  hidden_size = output_layer.shape[-1].value\n",
        "\n",
        "  # Create our own layer to tune for politeness data.\n",
        "  output_weights = tf.get_variable(\n",
        "      \"output_weights\", [num_labels, hidden_size],\n",
        "      initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.02))\n",
        "\n",
        "  output_bias = tf.get_variable(\n",
        "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
        "\n",
        "  with tf.variable_scope(\"loss\"):\n",
        "\n",
        "    # Dropout helps prevent overfitting\n",
        "    output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
        "\n",
        "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
        "    logits = tf.nn.bias_add(logits, output_bias)\n",
        "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "\n",
        "    # Convert labels into one-hot encoding\n",
        "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
        "\n",
        "    predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
        "    # If we're predicting, we want predicted labels and the probabiltiies.\n",
        "    if is_predicting:\n",
        "      return (predicted_labels, log_probs)\n",
        "\n",
        "    # If we're train/eval, compute loss between predicted and actual label\n",
        "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
        "    loss = tf.reduce_mean(per_example_loss)\n",
        "    return (loss, predicted_labels, log_probs)\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpE0ZIDOCQzE"
      },
      "source": [
        "Next we'll wrap our model function in a `model_fn_builder` function that adapts our model to work for training, evaluation, and prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnH-AnOQ9KKW"
      },
      "source": [
        "# model_fn_builder actually creates our model function\n",
        "# using the passed parameters for num_labels, learning_rate, etc.\n",
        "def model_fn_builder(num_labels, learning_rate, num_train_steps,\n",
        "                     num_warmup_steps):\n",
        "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
        "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
        "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
        "\n",
        "    input_ids = features[\"input_ids\"]\n",
        "    input_mask = features[\"input_mask\"]\n",
        "    segment_ids = features[\"segment_ids\"]\n",
        "    label_ids = features[\"label_ids\"]\n",
        "\n",
        "    is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
        "\n",
        "    # TRAIN and EVAL\n",
        "    if not is_predicting:\n",
        "\n",
        "      (loss, predicted_labels, log_probs) = create_model(\n",
        "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
        "\n",
        "      train_op = bert.optimization.create_optimizer(\n",
        "          loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n",
        "\n",
        "      # Calculate evaluation metrics.\n",
        "      def metric_fn(label_ids, predicted_labels):\n",
        "        accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n",
        "        f1_score = tf.contrib.metrics.f1_score(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        auc = tf.metrics.auc(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        recall = tf.metrics.recall(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        precision = tf.metrics.precision(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        true_pos = tf.metrics.true_positives(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        true_neg = tf.metrics.true_negatives(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        false_pos = tf.metrics.false_positives(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        false_neg = tf.metrics.false_negatives(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        return {\n",
        "            \"eval_accuracy\": accuracy,\n",
        "            \"f1_score\": f1_score,\n",
        "            \"auc\": auc,\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"true_positives\": true_pos,\n",
        "            \"true_negatives\": true_neg,\n",
        "            \"false_positives\": false_pos,\n",
        "            \"false_negatives\": false_neg\n",
        "        }\n",
        "\n",
        "      eval_metrics = metric_fn(label_ids, predicted_labels)\n",
        "\n",
        "      if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "        return tf.estimator.EstimatorSpec(mode=mode,\n",
        "          loss=loss,\n",
        "          train_op=train_op)\n",
        "      else:\n",
        "          return tf.estimator.EstimatorSpec(mode=mode,\n",
        "            loss=loss,\n",
        "            eval_metric_ops=eval_metrics)\n",
        "    else:\n",
        "      (predicted_labels, log_probs) = create_model(\n",
        "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
        "\n",
        "      predictions = {\n",
        "          'probabilities': log_probs,\n",
        "          'labels': predicted_labels\n",
        "      }\n",
        "      return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
        "\n",
        "  # Return the actual model function in the closure\n",
        "  return model_fn\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjwJ4bTeWXD8"
      },
      "source": [
        "# Compute train and warmup steps from batch size\n",
        "# These hyperparameters are copied from this colab notebook (https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 2e-5\n",
        "NUM_TRAIN_EPOCHS = 3.0\n",
        "# Warmup is a period of time where hte learning rate\n",
        "# is small and gradually increases--usually helps training.\n",
        "WARMUP_PROPORTION = 0.1\n",
        "# Model configs\n",
        "SAVE_CHECKPOINTS_STEPS = 500\n",
        "SAVE_SUMMARY_STEPS = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emHf9GhfWBZ_"
      },
      "source": [
        "# Compute # train and warmup steps from batch size\n",
        "num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEJldMr3WYZa"
      },
      "source": [
        "# Specify outpit directory and number of checkpoint steps to save\n",
        "run_config = tf.estimator.RunConfig(\n",
        "    model_dir=OUTPUT_DIR,\n",
        "    save_summary_steps=SAVE_SUMMARY_STEPS,\n",
        "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_WebpS1X97v"
      },
      "source": [
        "model_fn = model_fn_builder(\n",
        "  num_labels=len(label_list),\n",
        "  learning_rate=LEARNING_RATE,\n",
        "  num_train_steps=num_train_steps,\n",
        "  num_warmup_steps=num_warmup_steps)\n",
        "\n",
        "estimator = tf.estimator.Estimator(\n",
        "  model_fn=model_fn,\n",
        "  config=run_config,\n",
        "  params={\"batch_size\": BATCH_SIZE})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOO3RfG1DYLo"
      },
      "source": [
        "Next we create an input builder function that takes our training feature set (`train_features`) and produces a generator. This is a pretty standard design pattern for working with TensorFlow [Estimators](https://www.tensorflow.org/guide/estimators)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Pv2bAlOX_-K"
      },
      "source": [
        "# Create an input function for training. drop_remainder = True for using TPUs.\n",
        "train_input_fn = bert.run_classifier.input_fn_builder(\n",
        "    features=train_features,\n",
        "    seq_length=MAX_SEQ_LENGTH,\n",
        "    is_training=True,\n",
        "    drop_remainder=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6Nukby2EB6-"
      },
      "source": [
        "Now we train our model! For me, using a Colab notebook running on Google's GPUs, training time is typically 8-14 minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nucD4gluYJmK"
      },
      "source": [
        "print(f'Beginning Training!')\n",
        "current_time = datetime.now()\n",
        "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
        "print(\"Training took time \", datetime.now() - current_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmbLTVniARy3"
      },
      "source": [
        "Now let's use our test data to see how well our model did:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIhejfpyJ8Bx"
      },
      "source": [
        "test_input_fn = run_classifier.input_fn_builder(\n",
        "    features=test_features,\n",
        "    seq_length=MAX_SEQ_LENGTH,\n",
        "    is_training=False,\n",
        "    drop_remainder=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPVEXhNjYXC-"
      },
      "source": [
        "estimator.evaluate(input_fn=test_input_fn, steps=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueKsULteiz1B"
      },
      "source": [
        "Now let's write code to make predictions on new sentences:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsrbTD2EJTVl"
      },
      "source": [
        "def getPrediction(in_sentences):\n",
        "  labels = [\"Negative\", \"Positive\"]\n",
        "  input_examples = [run_classifier.InputExample(guid=\"\", text_a = x, text_b = None, label = 0) for x in in_sentences] # here, \"\" is just a dummy label\n",
        "  input_features = run_classifier.convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "  predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=False)\n",
        "  predictions = estimator.predict(predict_input_fn)\n",
        "  return [(sentence, prediction['probabilities'], labels[prediction['labels']]) for sentence, prediction in zip(in_sentences, predictions)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-thbodgih_VJ"
      },
      "source": [
        "pred_sentences = [\n",
        "  \"That movie was absolutely awful\",\n",
        "  \"The acting was a bit lacking\",\n",
        "  \"The film was creative and surprising\",\n",
        "  \"Absolutely fantastic!\"\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrZmvZySKQTm"
      },
      "source": [
        "predictions = getPrediction(pred_sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXkRiEBUqN3n"
      },
      "source": [
        "Voila! We have a sentiment classifier!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERkTE8-7oQLZ"
      },
      "source": [
        "predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3Tg7c47vfLE"
      },
      "source": [
        "# Export the model\n",
        "\n",
        "We are now ready to export the model. The following code defines the serving input function and exports the model to `OUTPUT_DIR`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfXsdV4qtlpW"
      },
      "source": [
        "def serving_input_fn():\n",
        "    reciever_tensors = {\n",
        "        \"input_ids\": tf.placeholder(dtype=tf.int32,\n",
        "                                    shape=[1, MAX_SEQ_LENGTH])\n",
        "    }\n",
        "    features = {\n",
        "        \"input_ids\": reciever_tensors['input_ids'],\n",
        "        \"input_mask\": 1 - tf.cast(tf.equal(reciever_tensors['input_ids'], 0), dtype=tf.int32),\n",
        "        \"segment_ids\": tf.zeros(dtype=tf.int32, shape=[1, MAX_SEQ_LENGTH]),\n",
        "        \"label_ids\": tf.placeholder(tf.int32, [None], name='label_ids')\n",
        "    }\n",
        "    return tf.estimator.export.ServingInputReceiver(features, reciever_tensors)\n",
        "\n",
        "estimator._export_to_tpu = False\n",
        "estimator.export_saved_model(OUTPUT_DIR+\"/export\", serving_input_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIFTmUbcwI0w"
      },
      "source": [
        "# Upload the model to AWS\n",
        "\n",
        "Cortex loads models from AWS, so we need to upload the exported model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gByRzrnR_OBX"
      },
      "source": [
        "Set these variables to configure your AWS credentials and model upload path:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bdCOb3z0_Gh",
        "cellView": "form"
      },
      "source": [
        "AWS_ACCESS_KEY_ID = \"\" #@param {type:\"string\"}\n",
        "AWS_SECRET_ACCESS_KEY = \"\" #@param {type:\"string\"}\n",
        "S3_UPLOAD_PATH = \"s3://my-bucket/sentiment-analysis/bert\" #@param {type:\"string\"}\n",
        "\n",
        "import sys\n",
        "import re\n",
        "\n",
        "if AWS_ACCESS_KEY_ID == \"\":\n",
        "    print(\"\\033[91m{}\\033[00m\".format(\"ERROR: Please set AWS_ACCESS_KEY_ID\"), file=sys.stderr)\n",
        "\n",
        "elif AWS_SECRET_ACCESS_KEY == \"\":\n",
        "    print(\"\\033[91m{}\\033[00m\".format(\"ERROR: Please set AWS_SECRET_ACCESS_KEY\"), file=sys.stderr)\n",
        "\n",
        "else:\n",
        "    try:\n",
        "        bucket = re.search(\"s3://(.+?)/\", S3_UPLOAD_PATH).group(1)\n",
        "        key = re.search(\"s3://.+?/(.+)\", S3_UPLOAD_PATH).group(1)\n",
        "    except:\n",
        "        print(\"\\033[91m{}\\033[00m\".format(\"ERROR: Invalid s3 path (should be of the form s3://my-bucket/path/to/file)\"), file=sys.stderr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLT09hZr_bhm"
      },
      "source": [
        "Upload to S3:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCN3BINl2sKN"
      },
      "source": [
        "import os\n",
        "import boto3\n",
        "\n",
        "s3 = boto3.client(\"s3\", aws_access_key_id=AWS_ACCESS_KEY_ID, aws_secret_access_key=AWS_SECRET_ACCESS_KEY)\n",
        "\n",
        "for dirpath, _, filenames in os.walk(OUTPUT_DIR+\"/export\"):\n",
        "    for filename in filenames:\n",
        "        filepath = os.path.join(dirpath, filename)\n",
        "        filekey = os.path.join(key, filepath[len(OUTPUT_DIR+\"/export/\"):])\n",
        "        print(\"Uploading s3://{}/{} ...\".format(bucket, filekey), end = '')\n",
        "        s3.upload_file(filepath, bucket, filekey)\n",
        "        print(\" ✓\")\n",
        "\n",
        "print(\"\\nUploaded model export directory to \" + S3_UPLOAD_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XPKSHzf_d7M"
      },
      "source": [
        "<!-- CORTEX_VERSION_MINOR -->\n",
        "That's it! See the [example on GitHub](https://github.com/cortexlabs/cortex/tree/0.8/examples/sentiment-analysis) for how to deploy the model as an API."
      ]
    }
  ]
}